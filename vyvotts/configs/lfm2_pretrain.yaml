# Model
model_name: "LiquidAI/LFM2-350M"
tokenizer_name: "LiquidAI/LFM2-350M"

# Training Args
epochs: 1
batch_size: 16
number_processes: 8
pad_token: 64407
save_steps: 12000
learning_rate: 5.0e-5
ratio: "2:1"

# Datasets
text_QA_dataset: "OpenSpeechHub/Emilia-EN-LFM2"
TTS_dataset: "OpenSpeechHub/Emilia-EN-LFM2"

# Naming and paths
save_folder: "checkpoints"
project_name: "lfm2-tts-transformers"
run_name: "lfm2-tts-transformers"
